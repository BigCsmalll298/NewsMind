import os
import sys
import torch
import re
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from collections import defaultdict
from typing import Dict, List
from tool_agents.search import NewsSearchAgent
from tool_agents.integrator import IntegrationAgent
from tool_agents.mailer import EmailAgent
from tool_agents.retriever import RAGAgent

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from tool_agents.base_agent import BaseAgent

class NewsMindAgent(BaseAgent):
    """
    NewsMindAgent ä½œä¸ºç³»ç»Ÿçš„è°ƒåº¦æ ¸å¿ƒï¼Œè´Ÿè´£åè°ƒæ–°é—»æ£€ç´¢ã€å†…å®¹æ•´åˆã€æ˜¯å¦è°ƒç”¨ RAG ä¸é‚®ä»¶å‘é€ã€‚
    ä½¿ç”¨å¤§æ¨¡å‹ Function Call å†³ç­–æ˜¯å¦ä½¿ç”¨ RAG æŸ¥è¯¢ã€‚
    """

    def __init__(self, use_collector: bool = False, max_articles: int = 20, clear_log: bool = False):
        super().__init__()
        self.logger.info(f"[NewsMindAgent] æ­£åœ¨åˆå§‹åŒ–...")
        self.use_collector = use_collector
        self.max_articles = max_articles
        self.clear_log = clear_log

        # åˆå§‹åŒ– Hugging Face æ¨¡å‹
        self.model_name = "deepseek-ai/deepseek-llm-7b-chat"
        self.logger.info(f"æ­£åœ¨åŠ è½½NewsMindçš„å¤§è„‘llmï¼š{self.model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        self.model.eval()
        self.logger.info(f"NewsMindçš„å¤§è„‘llmåŠ è½½å®Œæˆ!")

        self.search_agent = NewsSearchAgent()
        self.rag_agent = RAGAgent()
        self.integrate_agent = IntegrationAgent()
        self.email_agent = EmailAgent()
        self.logger.info(f"[NewsMindAgent] åˆå§‹åŒ–å®Œæˆ")

        if self.clear_log:
            self._clear_log_file()

    def _clear_log_file(self):
        log_path = os.path.abspath(os.path.join("log", "logs", "NewsMind.log"))
        if os.path.exists(log_path):
            with open(log_path, "w") as f:
                f.truncate()
            self.logger.info(f"âœ… å·²æ¸…ç©ºæ—¥å¿—æ–‡ä»¶: {log_path}")
        else:
            self.logger.info(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")

    def _llm_should_use_rag(self, user_query: str) -> bool:
        """
        ç”¨ Hugging Face ä¸Šçš„ DeepSeek æ¨¡å‹åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ RAG
        """
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆRAGï¼‰å›ç­”ã€‚
        åªå›ç­” True æˆ– Falseï¼Œä¸”åªèƒ½è¾“å‡ºè¿™ä¸¤ä¸ªå•è¯ä¹‹ä¸€ï¼Œä¸è¦å¤šä½™æ–‡å­—ã€‚

        è§„åˆ™ï¼š
        - å«â€œä»Šå¤©â€ã€â€œæœ€æ–°â€ã€â€œç°åœ¨â€ -> False
        - å«â€œæœ€è¿‘â€ã€â€œè‡ª...ä»¥æ¥â€ -> True

        ç¤ºä¾‹ï¼š
        é—®ï¼šå¸®æˆ‘æŸ¥ä»Šå¤©çš„æ–°é—»ã€‚ ç­”ï¼šFalse
        é—®ï¼šå¸®æˆ‘æŸ¥æœ€æ–°çš„æ–°é—»ã€‚ ç­”ï¼šFalse
        é—®ï¼šå¸®æˆ‘æŸ¥æœ€è¿‘çš„ç§‘æŠ€æ–°é—»ã€‚ ç­”ï¼šTrue
        é—®ï¼šå¸®æˆ‘æŸ¥è‡ªä»Šå¹´ä»¥æ¥çš„è´¢ç»æ–°é—»ã€‚ ç­”ï¼šTrue

        ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š
        {user_query}

        å›ç­”ï¼š
        """

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.8
            )
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        self.logger.info(f"ğŸ“¨ DeepSeek è¿”å›å†…å®¹: {answer}")

        match = re.search(r'(\b\w+)\W*$', answer)
        if match:
            last_word = match.group(1)
        else:
            last_word = None

        self.logger.info(f"æ¨¡å‹å›ç­”ï¼š{last_word}")
        if last_word == "True":
            return True
        elif last_word == "False":
            return False
        else:
            self.logger.warning(f"æœªçŸ¥ç­”æ¡ˆ: {answer}, é»˜è®¤Falseï¼ï¼ï¼")
            return False
        


    
    def group_by_domain(self, articles_list: List[Dict]) -> Dict[str, List[Dict]]:
        """
        å°†RAGçš„ç»“æœç»„ç»‡æˆå­—å…¸ä»¥ä¾¿åç»­æ•´åˆ
        """
        articles_by_domain = defaultdict(list)
        for article in articles_list:
            domain = article.get("domain", "unknown")
            articles_by_domain[domain].append(article)
        return dict(articles_by_domain)
    

    def count_total_dicts(self,data: Dict[str, List[dict]]) -> int:
        """
        ç»Ÿè®¡æ–‡ç« æ•°é‡
        """
        return sum(len(lst) for lst in data.values())

    def run(self, user_query: str):
        self.logger.info("ğŸ§  [NewsMindAgent] å¯åŠ¨ä»»åŠ¡ ...")
        self.logger.info(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢å†…å®¹: {user_query}")

        articles = None #ç”¨æ¥å­˜å‚¨éœ€è¦å‘é€çš„æ–‡ç« 

        # Step 1: åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ RAG
        use_rag = self._llm_should_use_rag(user_query)
        if use_rag:
            self.logger.info("âœ… å¤§æ¨¡å‹åˆ¤æ–­ç»“æœï¼šä½¿ç”¨ RAG æŸ¥è¯¢")
            self.rag_agent.load_index_from_local("data/qdrant/storage")
            rag_answer = self.rag_agent.query(user_query)
            self.logger.info("ğŸ¯ RAG æŸ¥è¯¢ç»“æœï¼š")
            for r in rag_answer:  
                self.logger.info(json.dumps(r, ensure_ascii=False, indent=2))
            articles = self.group_by_domain(rag_answer)
        else:
            self.logger.info("ğŸš« å¤§æ¨¡å‹åˆ¤æ–­ç»“æœï¼šä¸ä½¿ç”¨ RAG")
            self.logger.info("ğŸ” å¼€å§‹æ–°é—»æ£€ç´¢é˜¶æ®µ ...")
            unfiltered_articles, filtered_articles = self.search_agent.run(
                max_articles=self.max_articles,
                use_collector=self.use_collector
            )
            self.logger.info(f"ğŸ” æ£€ç´¢åˆ°åŸå§‹æ–°é—»æ•°é‡: {self.count_total_dicts(unfiltered_articles)}ï¼Œç­›é€‰åçš„æ–°é—»æ•°é‡: {self.count_total_dicts(filtered_articles)}")
            articles = filtered_articles      

            #å°†æœ€è¿‘æœç´¢åˆ°çš„æ–°é—»å‘é‡è¿½åŠ å­˜å‚¨åˆ°å‘é‡åº“
            self.logger.info("ğŸ“¦ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼• ...")
            items = self.rag_agent.load_news_items(unfiltered_articles)
            
            self.rag_agent.build_index(
                news_items=items,
                client_dir="data/qdrant/client",
                vector_store_collection_name="news_collection",
                save_dir="data/qdrant/storage"
            )
            self.logger.info("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆã€‚")

        # Step 2: æ•´åˆå†…å®¹
        self.logger.info("ğŸ§© å¼€å§‹æ•´åˆ HTML é‚®ä»¶å†…å®¹ ...")
        html_content = self.integrate_agent.integrate_content(
            filtered_articles=articles,
            max_articles=self.max_articles
        )
        self.logger.info("âœ… é‚®ä»¶å†…å®¹æ•´åˆå®Œæˆã€‚")

        # Step 3: é‚®ä»¶å‘é€
        self.logger.info("ğŸ“¨ å¼€å§‹å‘é€é‚®ä»¶ ...")
        self.email_agent.send_email(html_content)
        self.logger.info("âœ… é‚®ä»¶å‘é€å®Œæˆï¼Œä»»åŠ¡ç»“æŸã€‚")
